questions:
# Ollama Settings Group - Priority settings for the AI model server
- variable: advancedConfig
  group: "Ollama Settings"
  label: "Show Advanced Configuration Options"
  description: "Enable to show advanced settings like Images, Pipeline Integration, and Automation. Keep disabled for simplified deployment."
  type: "boolean"
  default: false

- variable: ollama.gpu.enabled
  group: "Ollama Settings"
  label: "Enable NVIDIA GPU Acceleration"
  description: "Requires a node with NVIDIA drivers and the k8s device plugin installed."
  type: "boolean"
  default: false

- variable: ollama.hardware.type
  group: "Ollama Settings"
  label: "Target Hardware for Ollama"
  description: "Select the target hardware architecture for the Ollama deployment."
  type: "enum"
  required: true
  default: "nvidia"
  options:
    - label: "NVIDIA GPU (amd64)"
      value: "nvidia"
    - label: "Apple Silicon / CPU (arm64)"
      value: "apple"

- variable: ollama.resources.requests.cpu
  group: "Ollama Settings"
  label: "Ollama CPU Request"
  description: "The amount of CPU to reserve for the main Ollama container (e.g., '500m', '1')."
  type: "string"
  default: "2"
  show_if: "advancedConfig=true"

- variable: ollama.resources.requests.memory
  group: "Ollama Settings"
  label: "Ollama Memory Request"
  description: "The amount of memory to reserve for the main Ollama container (e.g., '1Gi', '2048Mi')."
  type: "string"
  default: "2Gi"
  show_if: "advancedConfig=true"

- variable: ollama.resources.limits.cpu
  group: "Ollama Settings"
  label: "Ollama CPU Limit"
  description: "The maximum amount of CPU the main Ollama container can use."
  type: "string"
  default: "4"
  show_if: "advancedConfig=true"

- variable: ollama.resources.limits.memory
  group: "Ollama Settings"
  label: "Ollama Memory Limit"
  description: "The maximum amount of memory the main Ollama container can use."
  type: "string"
  default: "16Gi"
  show_if: "advancedConfig=true"

- variable: ollama.persistence.enabled
  group: "Ollama Settings"
  label: "Enable Ollama Model Persistence"
  description: "If true, a PersistentVolumeClaim will be created to store Ollama models."
  type: "boolean"
  default: false

- variable: ollama.persistence.size
  group: "Ollama Settings"
  label: "Ollama Model Storage Size"
  description: "The size of the PersistentVolumeClaim for Ollama models (e.g., '10Gi', '50Gi')."
  type: "string"
  default: "10Gi"
  show_if: "ollama.persistence.enabled=true"

- variable: ollama.persistence.storageClassName
  group: "Ollama Settings"
  label: "Ollama Model Storage Class Name"
  description: "The storage class name for the Ollama models PVC. Leave empty to use the default storage class."
  type: "string"
  default: ""
  show_if: "ollama.persistence.enabled=true"

- variable: ollama.modelCache.nfs.enabled
  group: "Ollama Settings"
  label: "Enable NFS Model Caching"
  description: "Use NFS share to cache AI models across deployments for faster startup times. Models are downloaded once and shared across all pods."
  type: "boolean"
  default: false

- variable: ollama.modelCache.nfs.server
  group: "Ollama Settings"
  label: "NFS Server IP/Hostname"
  description: "NFS server hostname or IP address (e.g., 10.0.9.10)"
  type: "string"
  default: "10.0.9.10"
  show_if: "ollama.modelCache.nfs.enabled=true"

- variable: ollama.modelCache.nfs.path
  group: "Ollama Settings"
  label: "NFS Share Path"
  description: "Path to the NFS share directory for caching models (e.g., /data/ai-models)"
  type: "string"
  default: "/data/ai-models"
  show_if: "ollama.modelCache.nfs.enabled=true"

# Networking Group - For configuring how services are exposed
- variable: llmChat.service.type
  group: "Networking"
  label: "AI Compare Service Type"
  description: "Method to expose the AI Compare service."
  type: "enum"
  required: true
  default: "NodePort"
  options:
    - "ClusterIP"
    - "NodePort"
    - "LoadBalancer"

- variable: openWebui.service.type
  group: "Networking"
  label: "Open WebUI Service Type"
  description: "Method to expose the Open WebUI service."
  type: "enum"
  required: true
  default: "NodePort"
  options:
    - "ClusterIP"
    - "NodePort"
    - "LoadBalancer"

- variable: ollama.service.type
  group: "Networking"
  label: "Ollama Service Type"
  description: "Method to expose the Ollama service. 'ClusterIP' is recommended."
  type: "enum"
  required: true
  default: "ClusterIP"
  options:
    - "ClusterIP"
    - "NodePort"
    - "LoadBalancer"

# Pipeline Integration Group
- variable: pipelines.enabled
  group: "Pipeline Integration"
  label: "Enable Open WebUI Pipelines"
  description: "Enable pipeline processing for response modification and enhanced AI capabilities."
  type: "boolean"
  default: true
  show_if: "advancedConfig=true"

- variable: pipelines.config.pipelineMode
  group: "Pipeline Integration"
  label: "Pipeline Mode"
  description: "How the response level pipeline operates"
  type: "enum"
  default: "auto-cycle"
  options:
    - label: "Auto-Cycle (cycles through all levels automatically)"
      value: "auto-cycle"
    - label: "Manual (stays on selected level)"
      value: "manual"
  show_if: "advancedConfig=true&&pipelines.enabled=true"

- variable: pipelines.git.enabled
  group: "Pipeline Integration"
  label: "Auto-Deploy Pipelines from Git"
  description: "Automatically download and deploy pipelines from the git repository"
  type: "boolean"
  default: true
  show_if: "advancedConfig=true&&pipelines.enabled=true"

- variable: pipelines.autoConfig.enabled
  group: "Pipeline Integration"
  label: "Auto-Configure Open WebUI Connection"
  description: "Automatically set up pipeline connection in Open WebUI during deployment"
  type: "boolean"
  default: true
  show_if: "advancedConfig=true&&pipelines.enabled=true"

- variable: pipelines.autoConfig.apiKey
  group: "Pipeline Integration"
  label: "Pipeline API Key"
  description: "API key for Open WebUI to connect to the pipeline service"
  type: "string"
  default: "0p3n-w3bu!"
  show_if: "advancedConfig=true&&pipelines.autoConfig.enabled=true"

# Images Group
- variable: ollama.image.tag
  group: "Images"
  label: "Ollama Image Tag"
  description: "The container image tag for Ollama."
  type: "string"
  default: "0.6.8"
  show_if: "advancedConfig=true"

- variable: openWebui.image.tag
  group: "Images"
  label: "Open WebUI Image Tag"
  description: "The container image tag for Open WebUI."
  type: "string"
  default: "0.6.9"
  show_if: "advancedConfig=true"

- variable: pipelines.image.tag
  group: "Images"
  label: "Open WebUI Pipelines Image Tag"
  description: "The container image tag for Open WebUI Pipelines."
  type: "string"
  default: "main"
  show_if: "advancedConfig=true"

- variable: llmChat.image.tag
  group: "Images"
  label: "AI Compare Image Tag"
  description: "The container image tag for the AI Compare App."
  type: "string"
  default: "latest"
  show_if: "advancedConfig=true"

# Automation Settings Group - Settings for the automated test runner
- variable: llmChat.automation.enabled
  group: "Automation Settings"
  label: "Enable Automated Runner"
  description: "If true, the application will automatically run a test on a timer to monitor provider status and test AI responses."
  type: "boolean"
  default: false
  show_if: "advancedConfig=true"

- variable: llmChat.automation.sendMessages
  group: "Automation Settings"
  label: "Send Test Messages to AI Models"
  description: "When enabled, automation will send test questions to both Ollama and Open WebUI."
  type: "boolean"
  default: true
  show_if: "advancedConfig=true&&llmChat.automation.enabled=true"

- variable: llmChat.automation.providerTest
  group: "Automation Settings"
  label: "Enable Model Provider Status Test"
  description: "When enabled, regularly tests connectivity to model providers and updates status."
  type: "boolean"
  default: false
  show_if: "advancedConfig=true&&llmChat.automation.enabled=true"

- variable: llmChat.automation.intervalSeconds
  group: "Automation Settings"
  label: "Automation Interval (Seconds)"
  description: "The time in seconds between each automated test run."
  type: "int"
  default: 30
  show_if: "advancedConfig=true&&llmChat.automation.enabled=true"

- variable: llmChat.automation.defaultPrompt
  group: "Automation Settings"
  label: "Automation Test Prompt"
  description: "The recurring prompt to send to AI models during automated testing."
  type: "string"
  default: "Why is the sky blue? Be concise."
  show_if: "advancedConfig=true&&llmChat.automation.sendMessages=true"

# Observability Settings Group - OpenTelemetry and monitoring configuration
- variable: llmChat.observability.enabled
  group: "Observability Settings"
  label: "Enable OpenTelemetry Observability"
  description: "Enable OpenLIT observability for LLM request tracing, token usage monitoring, and performance metrics."
  type: "boolean"
  default: false

- variable: llmChat.observability.otlpEndpoint
  group: "Observability Settings"
  label: "OpenTelemetry Collector Endpoint"
  description: "OTLP endpoint URL for sending telemetry data (e.g., http://opentelemetry-collector.suse-observability.svc.cluster.local:4318)"
  type: "string"
  default: "http://opentelemetry-collector.suse-observability.svc.cluster.local:4318"
  show_if: "llmChat.observability.enabled=true"

- variable: llmChat.observability.collectGpuStats
  group: "Observability Settings"
  label: "Enable GPU Statistics Collection"
  description: "Collect GPU utilization and memory usage statistics. Requires GPU nodes and proper scheduling."
  type: "boolean"
  default: false
  show_if: "llmChat.observability.enabled=true"

# Security Settings Group - NeuVector integration and DLP configuration
- variable: neuvector.enabled
  group: "Security Settings"
  label: "Enable NeuVector Integration"
  description: "Automatically configure NeuVector DLP sensors for security demo functionality. Requires NeuVector to be deployed in the cluster."
  type: "boolean"
  default: false
  show_if: "advancedConfig=true"

- variable: neuvector.controllerUrl
  group: "Security Settings"
  label: "NeuVector Controller URL"
  description: "URL to the NeuVector controller service for API configuration"
  type: "string"
  default: "https://neuvector-svc-controller.neuvector.svc.cluster.local:10443"
  show_if: "advancedConfig=true&&neuvector.enabled=true"

- variable: neuvector.username
  group: "Security Settings"
  label: "NeuVector Admin Username"
  description: "Administrative username for NeuVector API access"
  type: "string"
  default: "admin"
  show_if: "advancedConfig=true&&neuvector.enabled=true"

- variable: neuvector.password
  group: "Security Settings"
  label: "NeuVector Admin Password"
  description: "Administrative password for NeuVector API access"
  type: "password"
  default: "admin"
  show_if: "advancedConfig=true&&neuvector.enabled=true"