apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-compare-chat
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ai-compare-chat
  template:
    metadata:
      labels:
        app: ai-compare-chat
    spec:
      serviceAccountName: ai-compare-service-account
      containers:
        - name: chat-app
          image: "{{ .Values.llmChat.image.repository }}:{{ .Values.llmChat.image.tag }}"
          imagePullPolicy: {{ .Values.imagePullPolicy }}
          ports:
            - containerPort: {{ .Values.llmChat.service.port }}
          env:
            - name: OLLAMA_BASE_URL
              value: "http://ollama-service:11434"
            - name: OPEN_WEBUI_BASE_URL
              value: "http://open-webui-service:8080"
            # Pipeline service configuration
            - name: PIPELINES_BASE_URL
              value: "http://pipelines-service:9099"
            - name: PIPELINE_API_KEY
              value: "{{ .Values.pipelines.autoConfig.apiKey }}"
            # --- NEW ENVIRONMENT VARIABLES ---
            - name: AUTOMATION_ENABLED
              value: "{{ .Values.llmChat.automation.enabled }}"
            - name: AUTOMATION_PROMPT
              value: "{{ .Values.llmChat.automation.defaultPrompt }}"
            - name: AUTOMATION_INTERVAL
              value: "{{ .Values.llmChat.automation.intervalSeconds }}"
            - name: AUTOMATION_SEND_MESSAGES
              value: "{{ .Values.llmChat.automation.sendMessages }}"
            # Observability configuration
            - name: OBSERVABILITY_ENABLED
              value: "{{ .Values.llmChat.observability.enabled }}"
            - name: OTLP_ENDPOINT
              value: "{{ .Values.llmChat.observability.otlpEndpoint }}"
            - name: COLLECT_GPU_STATS
              value: "{{ .Values.llmChat.observability.collectGpuStats }}"
            # Model configuration for demo
            - name: MODEL_CONFIG
              valueFrom:
                configMapKeyRef:
                  name: ai-compare-config
                  key: MODEL_CONFIG
            - name: CONFIG_MAP_NAME
              value: "ai-compare-config"
            - name: CONFIG_MAP_NAMESPACE
              value: "{{ .Release.Namespace }}"
            # Service health failure simulation
            - name: SERVICE_HEALTH_FAILURE
              value: "false"
            - name: DEPLOYMENT_NAME
              value: "ai-compare-chat"
          # Health checks to ensure proper startup sequencing
          livenessProbe:
            httpGet:
              path: /
              port: {{ .Values.llmChat.service.port }}
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /
              port: {{ .Values.llmChat.service.port }}
            initialDelaySeconds: 15
            periodSeconds: 5
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 3
          volumeMounts:
            - name: config-volume
              mountPath: /app/config.json
              subPath: config.json
      volumes:
        - name: config-volume
          configMap:
            name: chat-app-config