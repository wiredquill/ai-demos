apiVersion: v1
kind: Service
metadata:
  name: ollama-service
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/component: llm-inference
    app.kubernetes.io/part-of: genai-stack
    ai.framework: "ollama"
    ai.model.type: "llm"
    genai.application: "true"
  annotations:
    genai.observability/enabled: "true"
    genai.observability/type: "inference-server"
    genai.observability/models: "llama3.2:latest"
    genai.observability/framework: "ollama"
    observability.suse.com/genai-app: "true"
    ai.application.category: "model-inference"
spec:
  selector:
    app: ollama
  ports:
  - name: http
    port: {{ .Values.ollama.service.port }}
    targetPort: {{ .Values.ollama.service.port }}
  type: {{ .Values.ollama.service.type }}