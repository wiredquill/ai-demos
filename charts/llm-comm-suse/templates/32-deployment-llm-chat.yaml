apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-chat-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama-chat-app
  template:
    metadata:
      labels:
        app: ollama-chat-app
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      containers:
        - name: chat-app
          image: "{{ .Values.llmChat.image.repository }}:{{ .Values.llmChat.image.tag }}"
          imagePullPolicy: {{ .Values.imagePullPolicy }}
          ports:
            - containerPort: {{ .Values.llmChat.service.port }}
          env:
            - name: OLLAMA_BASE_URL
              value: "http://ollama-service:11434"
            - name: OPEN_WEBUI_BASE_URL
              value: "http://open-webui-service:8080"
            # Pipeline service configuration
            - name: PIPELINES_BASE_URL
              value: "http://pipelines-service:9099"
            - name: PIPELINE_API_KEY
              value: "{{ .Values.pipelines.autoConfig.apiKey }}"
            # --- NEW ENVIRONMENT VARIABLES ---
            - name: AUTOMATION_ENABLED
              value: "{{ .Values.llmChat.automation.enabled }}"
            - name: AUTOMATION_PROMPT
              value: "{{ .Values.llmChat.automation.defaultPrompt }}"
            - name: AUTOMATION_INTERVAL
              value: "{{ .Values.llmChat.automation.intervalSeconds }}"
            - name: AUTOMATION_SEND_MESSAGES
              value: "{{ .Values.llmChat.automation.sendMessages }}"
            # Observability configuration
            - name: OBSERVABILITY_ENABLED
              value: "{{ .Values.llmChat.observability.enabled }}"
            - name: OTLP_ENDPOINT
              value: "{{ .Values.llmChat.observability.otlpEndpoint }}"
            - name: COLLECT_GPU_STATS
              value: "{{ .Values.llmChat.observability.collectGpuStats }}"
          # Health checks to ensure proper startup sequencing
          livenessProbe:
            httpGet:
              path: /
              port: {{ .Values.llmChat.service.port }}
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /
              port: {{ .Values.llmChat.service.port }}
            initialDelaySeconds: 15
            periodSeconds: 5
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 3
          volumeMounts:
            - name: config-volume
              mountPath: /app/config.json
              subPath: config.json
      volumes:
        - name: config-volume
          configMap:
            name: chat-app-config