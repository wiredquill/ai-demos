# Default values for llm-comm-unified.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Common settings
llmChat:
  service:
    type: NodePort
    port: 80
  image:
    repository: "" # Placeholder, will be set by codebase selection
    tag: "" # Placeholder, will be set by codebase selection

openWebui:
  service:
    type: NodePort
  image:
    repository: "" # Placeholder, will be set by codebase selection
    tag: "" # Placeholder, will be set by codebase selection

ollama:
  service:
    type: ClusterIP
  hardware:
    type: nvidia
  gpu:
    enabled: false
  resources:
    requests:
      cpu: "2"
      memory: "2Gi"
    limits:
      cpu: "4"
      memory: "16Gi"
  persistence:
    enabled: false
    size: "10Gi"
    storageClassName: ""
  image:
    repository: "" # Placeholder, will be set by codebase selection
    tag: "" # Placeholder, will be set by codebase selection

# Deployment Options
enableSuseDeployment: true
enableUpstreamDeployment: true
suseDeploymentSuffix: "-suse"
upstreamDeploymentSuffix: "-upstream"

# Image repositories and tags for SUSE and Upstream
suse:
  llmChat:
    image:
      repository: "ghcr.io/wiredquill/ai-demos-suse"
      tag: "latest"
  openWebui:
    image:
      repository: "dp.apps.rancher.io/containers/open-webui"
      tag: "0.6.9"
  ollama:
    image:
      repository: "dp.apps.rancher.io/containers/ollama"
      tag: "0.6.8"

upstream:
  llmChat:
    image:
      repository: "ghcr.io/wiredquill/ai-demos"
      tag: "latest"
  openWebui:
    image:
      repository: "ghcr.io/open-webui/open-webui"
      tag: "0.6.9"
  ollama:
    image:
      repository: "ollama/ollama"
      tag: "0.6.8"

# Global settings
imagePullPolicy: Always
imagePullSecrets:
  - name: application-collection
