Demo 3: Monitoring AI with SUSE Observability
Goal: Highlight how SUSE Observability provides deep insights into the performance and cost of AI workloads, including GPU utilization and API token consumption.

Key Steps:

Access the SUSE Observability dashboards for the managed cluster.

Display the GPU monitoring dashboard. Point out metrics like GPU utilization, memory usage, and temperature for specific pods.

Show a custom dashboard that is tracking API token usage for a Large Language Model (LLM). This can be simulated by showing a metric that increments with each API call.

Explain how these metrics, viewed through SUSE Observability, are crucial for performance tuning and cost management (chargeback/showback).

What to Look For:

Real-time, granular visibility into GPU performance within the unified SUSE Observability UI.

The ability to track application-specific metrics like LLM token counts.

How this level of observability empowers teams to optimize resource usage and control costs effectively.