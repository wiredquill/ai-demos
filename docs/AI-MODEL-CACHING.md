# AI Model Caching Solutions

This document outlines various approaches to cache AI models to avoid downloading large models (600MB+) on every cluster deployment.

## ğŸš€ Solution 1: Pre-built Docker Image (IMPLEMENTED)

**Status**: âœ… Implemented  
**Speed**: Fastest (~30 seconds vs ~5 minutes)  
**Maintenance**: Low

### How it works:
- Custom Docker image with TinyLlama model pre-installed
- No initContainer downloads needed
- Models are ready immediately on pod start

### Build Process:
```bash
# Image built automatically by GitHub Actions
docker build -f app/Dockerfile.ollama-tinyllama -t ghcr.io/suse-technical-marketing/ollama-tinyllama:latest .
```

### Benefits:
- âœ… **Instant startup** - no model download wait
- âœ… **Reliable** - no network dependency during deployment
- âœ… **Simple** - standard Docker image pull
- âœ… **Version controlled** - model version tied to image tag

### Drawbacks:
- âŒ **Image size** - larger Docker image (~1.2GB vs 600MB)
- âŒ **Model updates** - requires rebuilding image for new models

---

## ğŸ—ï¸ Solution 2: Harbor OCI Registry (AVAILABLE)

**Status**: ğŸŸ¡ Available for Harbor v2.5+  
**Speed**: Fast (~1-2 minutes)  
**Maintenance**: Medium

### Prerequisites:
- Harbor v2.5+ with OCI artifact support
- ORAS CLI tool for pushing models

### Setup:
```bash
# 1. Configure Harbor project for AI models
# 2. Push model as OCI artifact
oras push harbor.yourdomain.com/ai-models/tinyllama:latest \
  ~/.ollama/models/blobs/* \
  --annotation "ai.model.type=ollama"

# 3. Configure Ollama to pull from Harbor
export OLLAMA_MODELS_REGISTRY="harbor.yourdomain.com/ai-models"
```

### Benefits:
- âœ… **Centralized** - all models in one registry
- âœ… **Versioning** - proper model version management
- âœ… **Access control** - Harbor RBAC for models
- âœ… **Replication** - Harbor multi-region support

### Drawbacks:
- âŒ **Harbor dependency** - requires Harbor v2.5+
- âŒ **Complexity** - additional OCI artifact management
- âŒ **Network dependency** - still downloads during deployment

---

## ğŸ’¾ Solution 3: Persistent Shared Storage (AVAILABLE)

**Status**: ğŸŸ¡ Available with NFS/EFS  
**Speed**: Medium (~2-3 minutes)  
**Maintenance**: High

### Setup:
```yaml
# Enable in values.yaml
ollama:
  modelCache:
    sharedStorage:
      enabled: true
      storageClassName: "nfs-client"
```

### Benefits:
- âœ… **Persistent** - survives cluster rebuilds
- âœ… **Shared** - multiple deployments use same cache
- âœ… **Cost effective** - reuse existing NFS

### Drawbacks:
- âŒ **NFS dependency** - requires shared storage setup
- âŒ **Performance** - network storage latency
- âŒ **Complexity** - PVC lifecycle management

---

## ğŸ¯ Recommendation

**Use Solution 1 (Pre-built Image)** for:
- âœ… Demo environments
- âœ… Fixed model requirements
- âœ… Fastest deployment times
- âœ… Simplest maintenance

**Use Solution 2 (Harbor)** for:
- âœ… Production environments
- âœ… Multiple model versions
- âœ… Existing Harbor infrastructure
- âœ… Model governance requirements

**Use Solution 3 (Shared Storage)** for:
- âœ… Dynamic model loading
- âœ… Large model collections
- âœ… Development environments
- âœ… Cost optimization

## Configuration

### Current Implementation (Solution 1):
```yaml
# charts/ai-compare/values.yaml
ollama:
  image:
    repository: ghcr.io/suse-technical-marketing/ollama-tinyllama
    tag: "latest"
  modelCache:
    prebuiltImage: true  # Default: use pre-built image
```

### Alternative Configurations:
```yaml
# Harbor OCI Registry
ollama:
  modelCache:
    harbor:
      enabled: true
      registry: "harbor.example.com"
      project: "ai-models"

# Shared Storage
ollama:
  modelCache:
    sharedStorage:
      enabled: true
      storageClassName: "nfs-client"
```